{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Regresión logística y Naive Bayes- Detección de noticias falsas\n",
    "\n",
    "José Luis Aguilera Luzania"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introducción\n",
    "\n",
    "**¿Qué es la detección de *Fake News*?**\n",
    "La detección de noticias falsas (Fake News) es la tarea de evaluar la veracidad de las afirmaciones en las noticias. Este es un problema crítico en el Procesamiento del Lenguaje Natural (PLN) porque tanto en medios de noticias tradicionales como en medios digitales las Fake News generan un gran impacto social y político en cada individuo. Por ejemplo, la exposición a las Fake News puede generar actitudes de ineficacia, alienación y cinismo hacia ciertos candidatos políticos (Balmas, 2014).\n",
    "\n",
    "**Objetivo de la libreta**\n",
    "El objetivo de esta libreta clasificar las noticias utilizando los métodos de Regresión Logística y Naive Bayes Multinomial."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Librerías y datos\n",
    "\n",
    "**Librerías**\n",
    "- Manipulación de datos:\n",
    "    - `pandas`: Librería para manipular los datos de forma tabular.\n",
    "    - `matplotlib`: Librería para graficar.\n",
    "    - `cmd`: Librería para controlar el formato de impresión en la consola.\n",
    "    - `re`: Librería para utilizar expresiones regulares.\n",
    "\n",
    "- Procesamiento del lenguaje natural:\n",
    "    - `nltk`: Librería para utilizar técnicas de procesamiento del lenguaje natural.\n",
    "\n",
    "- Representación de los datos:\n",
    "    - `wordcloud`: Librería para generar una nube de palabras y guardarla como imagen *.png*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cmd\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Datos adicionales para la librería `nltk`\n",
    "- `punkt`: Necesario para utilizar el tokenizador de los textos.\n",
    "- `stopwords`: Palabras comunes que no añaden información, como: el, la, los, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JoseLuis_AL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JoseLuis_AL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Leer los datos\n",
    "\n",
    "Los datos están divididos en los archivos `train.xlsx` y `development.xlsx`, con 80% para entrenamiento y 20% para pruebas respectivamente.\n",
    "Para cargar los conjuntos de datos se utilizará la librería `pandas` y su estructura `DataFrame`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "   Categoria       Tema          Fuente  \\\n0      falsa  educación  el ruinaversal   \n1      falsa  educación     hay noticia   \n2      falsa  educación  el ruinaversal   \n3  verdadera  educación    el universal   \n4      falsa  educación          lamula   \n\n                                          Encabezado  \\\n0  rae incluira la palabra lady en el diccionario...   \n1               la palabra haiga aceptada por la rae   \n2  yordi rosado escribira y disenara los nuevos l...   \n3  unam capacitara a maestros para aprobar prueba...   \n4  pretenden aprobar libros escolares con conteni...   \n\n                                               Texto  \n0  rae incluira la palabra lady en el diccionario...  \n1  la palabra haiga aceptada por la rae la real a...  \n2  yordi rosado escribira y disenara los nuevos l...  \n3  unam capacitara a maestros para aprobar prueba...  \n4  alerta pretenden aprobar libros escolares con ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Categoria</th>\n      <th>Tema</th>\n      <th>Fuente</th>\n      <th>Encabezado</th>\n      <th>Texto</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>falsa</td>\n      <td>educación</td>\n      <td>el ruinaversal</td>\n      <td>rae incluira la palabra lady en el diccionario...</td>\n      <td>rae incluira la palabra lady en el diccionario...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>falsa</td>\n      <td>educación</td>\n      <td>hay noticia</td>\n      <td>la palabra haiga aceptada por la rae</td>\n      <td>la palabra haiga aceptada por la rae la real a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>falsa</td>\n      <td>educación</td>\n      <td>el ruinaversal</td>\n      <td>yordi rosado escribira y disenara los nuevos l...</td>\n      <td>yordi rosado escribira y disenara los nuevos l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>verdadera</td>\n      <td>educación</td>\n      <td>el universal</td>\n      <td>unam capacitara a maestros para aprobar prueba...</td>\n      <td>unam capacitara a maestros para aprobar prueba...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>falsa</td>\n      <td>educación</td>\n      <td>lamula</td>\n      <td>pretenden aprobar libros escolares con conteni...</td>\n      <td>alerta pretenden aprobar libros escolares con ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_datos = pd.read_csv('Datos/datos.csv')\n",
    "df_datos.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regresión logística\n",
    "\n",
    "La regresión logística es un modelo matemático empleado para predecir qué tan probable es que ocurra un evento teniendo en cuenta datos previos. Esta funciona con datos binarios, es decir, cuando un evento ocurre o no.\n",
    "\n",
    "![Regresión logística](./imagenes/regresion_logistica.png)\n",
    "\n",
    "Como se observa en la gráfica, $S-Curve$ representa una relación no lineal entre $X$ y $Y$. La letra $S$ significa sigmoide, ya que la función logística es una función sigmoide, la cual se representa con la siguiente ecuación:\n",
    "\n",
    "$$\n",
    "Y = \\frac{1}{1+e^{-Z}} \\\\\n",
    "Z = w.X+b\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Separar el texto de las categorías."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "X = df_datos['Texto']\n",
    "Y = df_datos['Categoria']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "Y = Y.astype('category').cat.codes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Obtener las stopwords.\n",
    "stopwords_spanish_nltk = stopwords.words('spanish')\n",
    "stopwords_spanish_json = list(pd.read_json('stopwords-es.json')[0])\n",
    "\n",
    "# Se concatenan las stopwords, se crea un set para eliminar repetidos y sé genera una lista.\n",
    "stopwords_spanish = list(set(stopwords_spanish_nltk+stopwords_spanish_json))\n",
    "\n",
    "# Se agrega la stopword 'NUMBER', debido a que no aporta nada a los datos para el análisis.\n",
    "stopwords_spanish.append('number')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Stemmer para palabras en español.\n",
    "snowball_stem = SnowballStemmer('spanish')\n",
    "\n",
    "def stemming(texto):\n",
    "\n",
    "    # Texto a minúsculas.\n",
    "    texto_stem = texto.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales.\n",
    "    texto_stem = unidecode.unidecode(texto)\n",
    "\n",
    "    # Agrupar solo las palabras.\n",
    "    texto_stem = re.findall(r'\\w+', texto_stem)\n",
    "    texto_stem = ' '.join(texto_stem)\n",
    "\n",
    "    # Separar las palabras.\n",
    "    texto_stem = texto_stem.split()\n",
    "\n",
    "    # Stemming.\n",
    "    texto_stem = [snowball_stem.stem(palabra) for palabra in texto_stem if not palabra in stopwords_spanish]\n",
    "\n",
    "    # Agrupar los tokens resultantes.\n",
    "    texto_stem = ' '.join(texto_stem)\n",
    "\n",
    "    return texto_stem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "X = X.apply(stemming)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "X = X.values\n",
    "Y = Y.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, stratify=Y, random_state=2021)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9957716701902748"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "training_data_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7142857142857143"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
    "test_data_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "La noticia es probablemente verdadera\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[3]\n",
    "prediction = model.predict(X_new)\n",
    "print(prediction)\n",
    "if (prediction[0]==1):\n",
    "  print('La noticia es probablemente verdadera')\n",
    "else:\n",
    "  print('La noticia es probablemente falsa')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def detect_news(content):\n",
    "    ## Aplicamos Stemming al texto\n",
    "    stemmed_data = [stemming(content)]\n",
    "    ## Lo convertimos en datos numéricos\n",
    "    vectorized_data = vectorizer.transform(stemmed_data)\n",
    "    ## Realizamos la predicción\n",
    "    prediction = model.predict(vectorized_data)\n",
    "    if (prediction[0]==1):\n",
    "      print('La noticia es probablemente verdadera')\n",
    "    else:\n",
    "      print('La noticia es probablemente falsa')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La noticia es probablemente verdadera\n"
     ]
    }
   ],
   "source": [
    "# Verdadera\n",
    "detect_news('El Gobierno de la Ciudad de México anunció el reforzamiento del programa ‘Conduce sin Alcohol’, con el cual se instalarán puntos de alcoholímetro en plazas comerciales, restaurantes y bares, así como en espacios públicos. Por órdenes de la jefa de Gobierno, Claudia Sheinbaum, se intensificará la presencia y operación del programa en las 16 alcaldías, incluyendo las entradas y salidas de la ciudad, donde se contará con la participación del personal de la Secretaría de Movilidad (Semovi). A partir de este 7 de diciembre y hasta el 16 de enero, habrá 23 puntos de prueba diarios: siete puntos para las jornadas diurnas, dos de ellos en zonas carreteras; y 16 para las jornadas nocturnas, con 10 puntos fijos y 6 itinerarios. En conferencia de prensa, el secretario de seguridad ciudadana de la CDMX, Omar García Harfuch, informó que se instalará alcoholímetro en plazas comerciales, en eventos públicos, durante eventos deportivos, en las explanadas de las alcaldías, así como en las zonas de restaurantes y bares. En esos lugares se instalarán dos puntos diarios de “pruebas amistosas”, en horarios de 12:00 a 18:00 horas y de 19:00 a 23:00. A su vez, la Secretaría de Seguridad Ciudadana fortalecerá los operativos viales en Anillo Periférico, Circuito Bicentenario y Calzada General Ignacio Zaragoza. García Harfuch señaló que para la implementación del operativo cuentan con herramientas tecnológicas nuevas, como lo son 94 videocámaras corporales y 30 equipos de alcostop, un medidor que permite detectar niveles de alcohol a distancia. El objetivo del reforzamiento de los puntos de alcoholímetro, indicó el secretario, es “salvaguardar la integridad física de conductores de vehículos, peatones, ciclistas y la ciudadanía en general, así como prevenir accidentes o hechos de tránsito ante el consumo inmoderado de alcohol”. También se realizarán “jornadas amistosas” con pláticas para prevenir accidentes relacionados con el consumo de alcohol. Estas se llevarán a cabo en plazas comerciales, parques públicos, explanadas de alcaldías, eventos deportivos, así como en restaurantes y bares.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La noticia es probablemente falsa\n"
     ]
    }
   ],
   "source": [
    "# Falsa\n",
    "detect_news('Ahora Ricardo Anaya pretende poner un Starbucks en cada municipio Los Whitexicans.- Apenas a unas semanas de que Ricardo Anaya anunciara su gira por mil municipios del país para “vivir los problemas como propios”, el excandidato ya se rindió. De acuerdo con fuentes cercanas al ¿político?, Anaya la estuvo pasando mal en los pueblos, pero la gota que derramó el vaso fue que no encontrara ningún Starbucks abierto. Ricardo Anaya estuvo de gira por comunidades rurales para ganarse la simpatía de la gente con miras a la elección presidencial de 2024. Y aunque parecía que al principio todo sería como un viaje normal de cualquier whitexican a un pueblo mágico, poco a poco se fue dando cuenta de la realidad de su situación. A la semana de iniciar el recorrido se le fue terminando el jamón ibérico, las latas de paté importado y todas sus otras provisiones finas. Sin embargo, el verdadero problema surgió cuando se terminó las botellas de café de Starbucks que había llevado consigo. Desesperado, Ricardo Anaya comenzó a preguntar en los pueblos por el Starbucks más cercano sin éxito. No aceptó un cafecito de olla Los pobladores le ofrecieron café de olla con piloncillo y canela, pero al estar acostumbrado a que su café lleve crema, chispas de chocolate y leche deslactosada, se vio en la necesidad de rechazarlo. Finalmente, desistió de su gira y renovó sus compromisos de precampaña. Ahora leerá los artículos de Wikipedia de mil municipios del país desde la comodidad de su oficina en Atlanta, que queda junto a un Starbucks.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}